__artifacts_v2__ = {
    "chromeWebHistory": {
        "name": "Web History",
        "description": "Parses web history from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/History*', '*/app_sbrowser/Default/History*', '*/app_opera/History*',
                  '*/Chromium/Default/History*'),
        "output_types": "standard",
    },
    "chromeWebVisits": {
        "name": "Web Visits",
        "description": "Parses web visits from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/History*', '*/app_sbrowser/Default/History*', '*/app_opera/History*',
                  '*/Chromium/Default/History*'),
        "output_types": "standard",
    },
    "chromeWebSearch": {
        "name": "Web Searches",
        "description": "Parses web searches from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/History*', '*/app_sbrowser/Default/History*', '*/app_opera/History*',
                  '*/Chromium/Default/History*'),
        "output_types": "standard",
    },
    "chromeDownloads": {
        "name": "Downloads",
        "description": "Parses downloads from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/History*', '*/app_sbrowser/Default/History*', '*/app_opera/History*',
                  '*/Chromium/Default/History*'),
        "output_types": "standard",
    },
    "chromeKeywordSearchTerms": {
        "name": "Keyword Search Terms",
        "description": "Parses keyword search terms from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/History*', '*/app_sbrowser/Default/History*', '*/app_opera/History*',
                  '*/Chromium/Default/History*'),
        "output_types": "standard",
    },
    "chromeAutofillEntries": {
        "name": "Autofill Entries",
        "description": "Parses autofill entries from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Web Data*', '*/app_sbrowser/Default/Web Data*', '*/app_opera/Web Data*',
                  '*/Chromium/Default/Web Data*'),
        "output_types": "standard",
    },
    "chromeAutofillProfiles": {
        "name": "Autofill Profiles",
        "description": "Parses Autofill Profiles from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Web Data*', '*/app_sbrowser/Default/Web Data*', '*/app_opera/Web Data*',
                  '*/Chromium/Default/Web Data*'),
        "output_types": "standard",
    },
    "chromeBookmarks": {
        "name": "Bookmarks",
        "description": "Parses Bookmarks from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "last_update_date": "2025-06-20",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Bookmarks*', '*/app_sbrowser/Default/Bookmarks*', '*/app_opera/Bookmarks*',
                  '*/Chromium/Default/Bookmarks*'),
        "output_types": "standard",
    },
    "chromeCookies": {
        "name": "Cookies",
        "description": "Parses Cookies from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Cookies*', '*/app_sbrowser/Default/Cookies*', '*/app_opera/Cookies*', '*/Chromium/Default/Cookies*'),
        "output_types": "standard",
    },
    "chromeLoginData": {
        "name": "Login Data",
        "description": "Parses Login Data from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Login Data*', '*/app_sbrowser/Default/Login Data*', '*/app_opera/Login Data*', '*/Chromium/Default/Login Data*'),
        "output_types": "standard",
    },
    "chromeTopSites": {
        "name": "Top Sites",
        "description": "Parses Top Sites from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Top Sites*', '*/app_sbrowser/Default/Top Sites*', '*/app_opera/Top Sites*', '*/Chromium/Default/Top Sites*'),
        "output_types": ['lava', 'tsv', 'html'],
    },
    "chromeOfflinePages": {
        "name": "Offline Pages",
        "description": "Parses Offline Pages from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Offline Pages/metadata/OfflinePages.db*',
                  '*/app_sbrowser/Default/Offline Pages/metadata/OfflinePages.db*',
                  '*/Chromium/Default/Offline Pages/metadata/OfflinePages.db*'),
        "output_types": "standard",
    },
    "chromeMediaHistorySessions": {
        "name": "Media History Sessions",
        "description": "Parses Media History Sessions from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Media History*', '*/app_sbrowser/Default/Media History*',
                  '*/app_opera/Media History*', '*/Chromium/Default/Media History*'),
        "output_types": "standard",
    },
    "chromeMediaHistoryPlaybacks": {
        "name": "Media History Playbacks",
        "description": "Parses Media History Playbacks from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Media History*', '*/app_sbrowser/Default/Media History*',
                  '*/app_opera/Media History*', '*/Chromium/Default/Media History*'),
        "output_types": "standard",
    },
    "chromeMediaHistoryOrigins": {
        "name": "Media History Origins",
        "description": "Parses Media History Origins from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Media History*', '*/app_sbrowser/Default/Media History*',
                  '*/app_opera/Media History*', '*/Chromium/Default/Media History*'),
        "output_types": "standard",
    },
    "chromeNetworkActionPredictor": {
        "name": "Network Action Predictor",
        "description": "Parses Network Action Predictor records from Chromium Based Browsers",
        "author": "@stark4n6",
        "version": "0.0.3",
        "date": "2024-11-10",
        "requirements": "none",
        "category": "Chromium",
        "notes": "",
        "paths": ('*/Chrome/Default/Network Action Predictor*','*/app_sbrowser/Default/Network Action Predictor*',
                  '*/app_opera/Network Action Predictor*', '*/Chromium/Default/Network Action Predictor*'),
        "output_types": ['lava', 'tsv', 'html'],
    },
}

import os
import textwrap
import urllib.parse
import json
import datetime
import re
from Crypto.Cipher import AES
from Crypto.Protocol.KDF import PBKDF2

from ileapp.scripts.artifact_report import ArtifactHtmlReport
from ileapp.scripts.ilapfuncs import logfunc, tsv, timeline, get_next_unused_name, open_sqlite_db_readonly, does_table_exist_in_db, does_column_exist_in_db, lava_process_artifact, lava_insert_sqlite_data, artifact_processor, convert_utc_human_to_timezone, convert_ts_human_to_utc


def get_browser_name(file_name):

    if 'brave' in file_name.lower():
        return 'Brave'
    elif 'microsoft' in file_name.lower():
        return 'Edge'
    elif 'opera' in file_name.lower():
        return 'Opera'
    elif 'chrome' in file_name.lower():
        return 'Chrome'
    else:
        return 'Unknown'


def decrypt(ciphertxt, key=b"peanuts"):
    if re.match(rb"^v1[01]",ciphertxt):
        ciphertxt = ciphertxt[3:]
    salt = b"saltysalt"
    derived_key = PBKDF2(key, salt, 0x10, 1)
    iv = b" "*0x10
    cipher = AES.new(derived_key, AES.MODE_CBC, IV=iv)
    try:
        plaintxt_pad = cipher.decrypt(ciphertxt)
        plaintxt = plaintxt_pad[:-ord(plaintxt_pad[len(plaintxt_pad)-1:])]
    except ValueError as ex:
        logfunc('Exception while decrypting data: ' + str(ex))
        plaintxt = b''
    return plaintxt


def get_valid_date(d1, d2):
    '''Returns a valid date based on closest year to now'''
    # Since the dates in question will be hundreds of years apart, this should be easy
    if d1 == '': return d2
    if d2 == '': return d1

    year1 = int(d1[0:4])
    year2 = int(d2[0:4])

    today = datetime.datetime.today()
    diff1 = abs(today.year - year1)
    diff2 = abs(today.year - year2)

    if diff1 < diff2:
        return d1
    else:
        return d2


@artifact_processor
def chromeWebHistory(files_found, report_folder, seeker, wrap_text, timezone_offset):

    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []
    data_headers = ['Last Visit Time', 'URL', 'Title', 'Visit Count', 'Typed Count', 'ID', 'Hidden']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'
    
    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'History':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'
            
        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        
        # Web History
        cursor.execute('''
        SELECT
        datetime(last_visit_time/1000000 + (strftime('%s','1601-01-01')),'unixepoch') AS LastVisitDate,
        url AS URL,
        title AS Title,
        visit_count AS VisitCount,
        typed_count AS TypedCount,
        id AS ID,
        CASE hidden
            WHEN 0 THEN ''
            WHEN 1 THEN 'Yes'
        END as Hidden
        FROM urls  
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Web History'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9] # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []

            for row in all_rows:
                dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]),timezone_offset)
                if wrap_text:
                    data_list.append((dt,textwrap.fill(row[1], width=100),row[2],row[3],row[4],row[5],row[6]))
                else:
                    data_list.append((dt,row[1],row[2],row[3],row[4],row[5],row[6]))
            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeWebHistory"
            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
        else:
            logfunc(f'No {browser_name} - Web History data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeWebVisits(files_found, report_folder, seeker, wrap_text, timezone_offset):

    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []
    data_headers = ['Visit Timestamp', 'URL', 'Title', 'Duration', 'Transition Type', 'Qualifier(s)', 'From Visit URL']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']
    
    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'History':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        #Web Visits
        cursor.execute('''
        SELECT
        datetime(visits.visit_time/1000000 + (strftime('%s','1601-01-01')),'unixepoch'),
        urls.url,
        urls.title,
        CASE visits.visit_duration
            WHEN 0 THEN ''
            ELSE strftime('%H:%M:%f', visits.visit_duration / 1000000.000,'unixepoch')
        END as Duration,
        CASE visits.transition & 0xff
            WHEN 0 THEN 'LINK'
            WHEN 1 THEN 'TYPED'
            WHEN 2 THEN 'AUTO_BOOKMARK'
            WHEN 3 THEN 'AUTO_SUBFRAME'
            WHEN 4 THEN 'MANUAL_SUBFRAME'
            WHEN 5 THEN 'GENERATED'
            WHEN 6 THEN 'START_PAGE'
            WHEN 7 THEN 'FORM_SUBMIT'
            WHEN 8 THEN 'RELOAD'
            WHEN 9 THEN 'KEYWORD'
            WHEN 10 THEN 'KEYWORD_GENERATED'
            ELSE NULL
        END AS CoreTransitionType,
        trim((CASE WHEN visits.transition & 0x00800000 THEN 'BLOCKED, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x01000000 THEN 'FORWARD_BACK, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x02000000 THEN 'FROM_ADDRESS_BAR, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x04000000 THEN 'HOME_PAGE, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x08000000 THEN 'FROM_API, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x10000000 THEN 'CHAIN_START, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x20000000 THEN 'CHAIN_END, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x40000000 THEN 'CLIENT_REDIRECT, ' ELSE '' END ||
        CASE WHEN visits.transition & 0x80000000 THEN 'SERVER_REDIRECT, ' ELSE '' END ||
        CASE WHEN visits.transition & 0xC0000000 THEN 'IS_REDIRECT_MASK, ' ELSE '' END),', ')
        AS Qualifiers,
        Query2.url AS FromURL
        FROM visits
        LEFT JOIN urls ON visits.url = urls.id
        LEFT JOIN (SELECT urls.url,urls.title,visits.visit_time,visits.id FROM visits LEFT JOIN urls ON visits.url = urls.id) Query2 ON visits.from_visit = Query2.id  
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Web Visits'
            report = ArtifactHtmlReport(report_name)
            #check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9] # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()

            data_list = []
            for row in all_rows:
                dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]),timezone_offset)
                if wrap_text:
                    data_list.append((dt,textwrap.fill(row[1], width=100),row[2],row[3],row[4],row[5],row[6]))
                else:
                    data_list.append((dt,row[1],row[2],row[3],row[4],row[5],row[6]))
            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeWebVisits"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Web Visits data available')

        db.close()

    return all_data_headers, all_data, report_file

@artifact_processor
def chromeWebSearch(files_found, report_folder, seeker, wrap_text, timezone_offset):

    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Visit Time', 'Search Term', 'URL', 'Title', 'Visit Count']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']
    
    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'History':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        #Web Search    
        cursor.execute('''
        SELECT
            url,
            title,
            visit_count,
            datetime(last_visit_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        FROM urls
        WHERE url like '%search?q=%'
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Web Search'
            report = ArtifactHtmlReport(report_name)
            #check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9] # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()

            data_list = []
            for row in all_rows:
                dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[3]),timezone_offset)
                search = row[0].split('search?q=')[1].split('&')[0]
                search = urllib.parse.unquote(search).replace('+', ' ')
                if wrap_text:
                    data_list.append((dt, search, (textwrap.fill(row[0], width=100)),row[1],row[2]))
                else:
                    data_list.append((dt, search, row[0], row[1], row[2]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()
            
            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeWebSearch"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Web Search Terms data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeDownloads(files_found, report_folder, seeker, wrap_text, timezone_offset):

    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Start Time', 'End Time', 'Last Access Time', 'URL', 'Target Path', 'State', 'Danger Type',
                    'Interrupt Reason', 'Opened?', 'Received Bytes', 'Total Bytes']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')
    lava_data_headers[1] = (lava_data_headers[1], 'datetime')
    lava_data_headers[2] = (lava_data_headers[2], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']
    
    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'History':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        #Downloads
        # check for last_access_time column, an older version of chrome db (32) does not have it
        if does_column_exist_in_db(file_found, 'downloads', 'last_access_time') == True:
            last_access_time_query = '''
            CASE last_access_time 
                WHEN "0" 
                THEN "" 
                ELSE datetime(last_access_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
            END AS "Last Access Time"'''
        else:
            last_access_time_query = "'' as last_access_query"

        cursor.execute(f'''
        SELECT 
        CASE start_time  
            WHEN "0" 
            THEN "" 
            ELSE datetime(start_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        END AS "Start Time", 
        CASE end_time 
            WHEN "0" 
            THEN "" 
            ELSE datetime(end_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        END AS "End Time", 
        {last_access_time_query},
        tab_url, 
        target_path, 
        CASE state
            WHEN "0" THEN "In Progress"
            WHEN "1" THEN "Complete"
            WHEN "2" THEN "Canceled"
            WHEN "3" THEN "Interrupted"
            WHEN "4" THEN "Interrupted"
        END,
        CASE danger_type
            WHEN "0" THEN ""
            WHEN "1" THEN "Dangerous"
            WHEN "2" THEN "Dangerous URL"
            WHEN "3" THEN "Dangerous Content"
            WHEN "4" THEN "Content May Be Malicious"
            WHEN "5" THEN "Uncommon Content"
            WHEN "6" THEN "Dangerous But User Validated"
            WHEN "7" THEN "Dangerous Host"
            WHEN "8" THEN "Potentially Unwanted"
            WHEN "9" THEN "Allowlisted by Policy"
            WHEN "10" THEN "Pending Scan"
            WHEN "11" THEN "Blocked - Password Protected"
            WHEN "12" THEN "Blocked - Too Large"
            WHEN "13" THEN "Warning - Sensitive Content"
            WHEN "14" THEN "Blocked - Sensitive Content"
            WHEN "15" THEN "Safe - Deep Scanned"
            WHEN "16" THEN "Dangerous, But User Opened"
            WHEN "17" THEN "Prompt For Scanning"
            WHEN "18" THEN "Blocked - Unsupported Type"
        END,
        CASE interrupt_reason
            WHEN "0" THEN ""
            WHEN "1" THEN "File Error"
            WHEN "2" THEN "Access Denied"
            WHEN "3" THEN "Disk Full"
            WHEN "5" THEN "Path Too Long"
            WHEN "6" THEN "File Too Large"
            WHEN "7" THEN "Virus"
            WHEN "10" THEN "Temporary Problem"
            WHEN "11" THEN "Blocked"
            WHEN "12" THEN "Security Check Failed"
            WHEN "13" THEN "Resume Error"
            WHEN "20" THEN "Network Error"
            WHEN "21" THEN "Operation Timed Out"
            WHEN "22" THEN "Connection Lost"
            WHEN "23" THEN "Server Down"
            WHEN "30" THEN "Server Error"
            WHEN "31" THEN "Range Request Error"
            WHEN "32" THEN "Server Precondition Error"
            WHEN "33" THEN "Unable To Get File"
            WHEN "34" THEN "Server Unauthorized"
            WHEN "35" THEN "Server Certificate Problem"
            WHEN "36" THEN "Server Access Forbidden"
            WHEN "37" THEN "Server Unreachable"
            WHEN "38" THEN "Content Lenght Mismatch"
            WHEN "39" THEN "Cross Origin Redirect"
            WHEN "40" THEN "Canceled"
            WHEN "41" THEN "Browser Shutdown"
            WHEN "50" THEN "Browser Crashed"
        END,
        opened, 
        received_bytes, 
        total_bytes
        FROM downloads
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Downloads'
            report = ArtifactHtmlReport(report_name)
            #check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9] # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                data_list.append((row[0],row[1],row[2],row[3],row[4],row[5],row[6],row[7],row[8],row[9],row[10]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeDownloads"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
        else:
            logfunc(f'No {browser_name} - Downloads data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeKeywordSearchTerms(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Visit Time','Term','URL']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']
    
    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'History':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        #Search Terms
        cursor.execute('''
        SELECT
            url_id,
            term,
            id,
            url,
            datetime(last_visit_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        FROM keyword_search_terms, urls
        WHERE url_id = id
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Keyword Search Terms'
            report = ArtifactHtmlReport(report_name)
            #check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9] # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[4]),timezone_offset)
                if wrap_text:
                    data_list.append((dt, row[1],(textwrap.fill(row[3], width=100))))
                else:
                    data_list.append((dt, row[1], row[3]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeKeywordSearchTerms"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
        else:
            logfunc(f'No {browser_name} - Keyword Search Terms data available')
        
        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeAutofillEntries(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Date Created', 'Field', 'Value', 'Date Last Used', 'Count']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')
    lava_data_headers[3] = (lava_data_headers[3], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    category = "Chromium"
    module_name = "chromeAutofillEntries"
    
    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'Web Data':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        columns = [i[1] for i in cursor.execute('PRAGMA table_info(autofill)')]
        # TODO: Was this supposed to be if 'date_last_used' in columns?
        if 'date_created' in columns:
            cursor.execute(f'''
            select
                datetime(date_created, 'unixepoch'),
                name,
                value,
                datetime(date_last_used, 'unixepoch'),
                count
            from autofill
            ''')

            all_rows = cursor.fetchall()
            if len(all_rows) > 0:
                report_name = f'{browser_name} - Autofill - Entries'
                report = ArtifactHtmlReport(report_name)
                # check for existing and get next name for report file, so report from another file does not get overwritten
                report_path = os.path.join(report_folder, f'{report_name}.temphtml')
                report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
                report.start_artifact_report(report_folder, os.path.basename(report_path))
                report.add_script()

                data_list = []
                for row in all_rows:
                    created_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                    last_used_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[3]), timezone_offset)
                    data_list.append((created_dt, row[1], row[2], last_used_dt, row[4]))

                report.write_artifact_data_table(data_headers, data_list, file_found)
                report.end_artifact_report()

                # Generate LAVA output

                table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                               lava_data_headers, len(data_list))

                lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

                # Add browser name column to the data
                data_list = [row + (browser_name,) for row in data_list]

                # Add current list to the combined list
                all_data.extend(data_list)

            else:
                logfunc(f'No {browser_name} - Autofill - Entries data available')

        else:
            cursor.execute(f'''
            select
                datetime(autofill_dates.date_created, 'unixepoch'),
                autofill.name,
                autofill.value,
                autofill.count
            from autofill
            join autofill_dates on autofill_dates.pair_id = autofill.pair_id
            ''')

            all_rows = cursor.fetchall()
            if len(all_rows) > 0:
                report_name = f'{browser_name} - Autofill - Entries'
                report = ArtifactHtmlReport(report_name)
                # check for existing and get next name for report file, so report from another file does not get overwritten
                report_path = os.path.join(report_folder, f'{report_name}.temphtml')
                report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
                report.start_artifact_report(report_folder, os.path.basename(report_path))
                report.add_script()
                data_list = []
                for row in all_rows:
                    created_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                    data_list.append((created_dt, row[1], row[2], None, row[3]))

                report.write_artifact_data_table(data_headers, data_list, file_found)
                report.end_artifact_report()

                # Generate LAVA output

                table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                               lava_data_headers, len(data_list))

                lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

                # Add browser name column to the data
                data_list = [row + (browser_name,) for row in data_list]

                # Add current list to the combined list
                all_data.extend(data_list)

            else:
                logfunc(f'No {browser_name} - Autofill - Entries data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeAutofillProfiles(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Date Modified', 'GUID', 'First Name', 'Middle Name', 'Last Name', 'Email', 'Phone Number',
                    'Company Name', 'Address', 'City', 'State', 'Zip Code', 'Date Last Used', 'Use Count']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']
    
    report_file = 'Unknown'
    
    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'Web Data':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found
        
        db = open_sqlite_db_readonly(file_found)
        
        if does_table_exist_in_db(file_found, 'autofill_profiles'):
            cursor = db.cursor()

            cursor.execute(f'''
            select
                datetime(date_modified, 'unixepoch'),
                autofill_profiles.guid,
                autofill_profile_names.first_name,
                autofill_profile_names.middle_name,
                autofill_profile_names.last_name,
                autofill_profile_emails.email,
                autofill_profile_phones.number,
                autofill_profiles.company_name,
                autofill_profiles.street_address,
                autofill_profiles.city,
                autofill_profiles.state,
                autofill_profiles.zipcode,
                datetime(use_date, 'unixepoch'),
                autofill_profiles.use_count
            from autofill_profiles
            inner join autofill_profile_emails ON autofill_profile_emails.guid = autofill_profiles.guid
            inner join autofill_profile_phones ON autofill_profiles.guid = autofill_profile_phones.guid
            inner join autofill_profile_names ON autofill_profile_phones.guid = autofill_profile_names.guid
            ''')


            all_rows = cursor.fetchall()
            if len(all_rows) > 0:
                report_name = f'{browser_name} - Autofill - Profiles'
                report = ArtifactHtmlReport(report_name)
                # check for existing and get next name for report file, so report from another file does not get overwritten
                report_path = os.path.join(report_folder, f'{report_name}.temphtml')
                report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
                report.start_artifact_report(report_folder, os.path.basename(report_path))
                report.add_script()

                data_list = []
                for row in all_rows:
                    modified_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                    last_used_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[12]), timezone_offset)

                    data_list.append((modified_dt, row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9],
                                      row[10], row[11], last_used_dt, row[13]))

                report.write_artifact_data_table(data_headers, data_list, file_found)
                report.end_artifact_report()

                # Generate LAVA output

                category = "Chromium"
                module_name = "chromeAutofillProfiles"

                table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                               lava_data_headers, len(data_list))

                lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

                # Add browser name column to the data
                data_list = [row + (browser_name,) for row in data_list]

                # Add current list to the combined list
                all_data.extend(data_list)

            else:
                logfunc(f'No {browser_name} - Autofill - Profiles data available')
        else:
            logfunc(f'No {browser_name} - Autofill - Profiles data available')
        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeBookmarks(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Added Date', 'URL', 'Name', 'Parent', 'Type']

    lava_data_headers = data_headers.copy()

    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        file_path = os.path.basename(file_found)

        if not (file_path == 'Bookmarks' or file_path == 'Bookmarks.bak'):  # skip -journal and other files
            continue

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        with open(file_found, "r") as f:
            dataa = json.load(f)
        data_list = []
        for x, y in dataa.items():
            children_items = list()
            if isinstance(y, dict):
                for key, value in y.items():
                    if isinstance(value, dict):
                        for keyb, valueb in value.items():
                            if keyb == 'children':
                                if len(valueb) > 0:
                                    for index in range(len(valueb)):
                                        url = valueb[index]['url']
                                        dateadd = valueb[index]['date_added']
                                        dateaddconv = datetime.datetime(1601, 1, 1) + datetime.timedelta(
                                            microseconds=int(dateadd))
                                        added_dt = convert_utc_human_to_timezone(dateaddconv, timezone_offset)
                                        name = valueb[0]['name']
                                        typed = valueb[0]['type']
                                        children_items.append((url, dateadd, dateaddconv, added_dt, name, typed))
                            if keyb == 'name' and len(children_items) > 0:
                                parent = valueb
                                for (url, _, _, added_dt, name, typed) in children_items:
                                    data_list.append((added_dt, url, name, parent, typed))
                                children_items = list()
        if len(data_list) > 0:
            report_name = f'{browser_name} - Bookmarks'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeBookmarks"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
        else:
            logfunc(f'No {browser_name} - Bookmarks data available')

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeCookies(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Access Date', 'Host', 'Name', 'Value', 'Created Date', 'Expiration Date', 'Path']

    lava_data_headers = data_headers.copy()

    lava_data_headers[0] = (lava_data_headers[0], 'datetime')
    lava_data_headers[4] = (lava_data_headers[4], 'datetime')
    lava_data_headers[5] = (lava_data_headers[5], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'Cookies':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        SELECT
        CASE
            last_access_utc 
            WHEN
                "0" 
            THEN
                "" 
            ELSE
                datetime(last_access_utc / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        END AS "last_access_utc", 
        host_key,
        name,
        value,
        CASE
            creation_utc 
            WHEN
                "0" 
            THEN
                "" 
            ELSE
                datetime(creation_utc / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        END AS "creation_utc", 
        CASE
            expires_utc 
            WHEN
                "0" 
            THEN
                "" 
            ELSE
                datetime(expires_utc / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
        END AS "expires_utc", 
        path
        FROM
        cookies
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Cookies'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                last_accessed_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                created_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[4]), timezone_offset)
                expiration_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[5]), timezone_offset)
                if wrap_text:
                    data_list.append(
                        (last_accessed_dt, row[1], (textwrap.fill(row[2], width=50)), row[3], created_dt, expiration_dt, row[6]))
                else:
                    data_list.append((last_accessed_dt, row[1], row[2], row[3], created_dt, expiration_dt, row[6]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeCookies"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Cookies data available')

        db.close()

    return all_data_headers, all_data, report_file

@artifact_processor
def chromeLoginData(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Created Time', 'Username', 'Password', 'Origin URL', 'Blacklisted by User', 'Browser Name']

    lava_data_headers = data_headers.copy()

    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'
    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'Login Data':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        SELECT
        username_value,
        password_value,
        CASE date_created 
            WHEN "0" THEN "" 
            ELSE datetime(date_created / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch")
            END AS "date_created_win_epoch", 
        CASE date_created WHEN "0" THEN "" 
            ELSE datetime(date_created / 1000000 + (strftime('%s', '1970-01-01')), "unixepoch")
            END AS "date_created_unix_epoch",
        origin_url,
        blacklisted_by_user
        FROM logins
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Login Data'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                password = ''
                password_enc = row[1]
                if password_enc:
                    password = decrypt(password_enc).decode("utf-8", 'replace')
                valid_date = get_valid_date(row[2], row[3])
                data_list.append((valid_date, row[0], password, row[4], row[5], browser_name))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeLoginData"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
        else:
            logfunc(f'No {browser_name} - Login Data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeTopSites(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['URL', 'Rank', 'Title', 'Redirects']

    lava_data_headers = data_headers.copy()

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'Top Sites':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        try:
            cursor.execute('''
            select
            url,
            url_rank,
            title,
            redirects
            FROM
            top_sites ORDER by url_rank asc
            ''')

            all_rows = cursor.fetchall()
            usageentries = len(all_rows)
        except:
            usageentries = 0

        if usageentries > 0:
            report_name = f'{browser_name} - Top Sites'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                data_list.append((row[0], row[1], row[2], row[3]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeTopSites"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Top Sites data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeOfflinePages(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Creation Time', 'Last Access Time', 'Online URL', 'File Path', 'Title', 'Access Count', 
                    'File Size']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')
    lava_data_headers[1] = (lava_data_headers[1], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not os.path.basename(file_found) == 'OfflinePages.db':  # skip -journal and other files
            continue
        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        SELECT
        datetime(creation_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch") as creation_time,
        datetime(last_access_time / 1000000 + (strftime('%s', '1601-01-01')), "unixepoch") as last_access_time,
        online_url,
        file_path,
        title,
        access_count,
        file_size
        from offlinepages_v1
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Offline Pages'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                created_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                last_accessed_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[1]), timezone_offset)
                if wrap_text:
                    data_list.append(
                        (created_dt, last_accessed_dt, (textwrap.fill(row[2], width=75)), row[3], row[4], row[5], row[6]))
                else:
                    data_list.append((created_dt, last_accessed_dt, row[2], row[3], row[4], row[5], row[6]))
            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeTopSites"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)
            
        else:
            logfunc(f'No {browser_name} - Offline Pages data available')

        db.close()
        
    return all_data_headers, all_data, report_file


@artifact_processor
def chromeMediaHistorySessions(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Updated', 'Origin ID', 'URL', 'Position', 'Duration', 'Title', 'Artist', 'Album',
                    'Source Title']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not file_found.endswith('Media History'):
            continue  # Skip all other files

        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        select
        datetime(last_updated_time_s-11644473600, 'unixepoch') as last_updated_time_s,
            origin_id,
            url,
            strftime('%H:%M:%S',position_ms/1000, 'unixepoch') as position_ms,
            strftime('%H:%M:%S',duration_ms/1000, 'unixepoch') as duration_ms,
            title,
            artist,
            album,
            source_title
        from playbackSession
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Media History - Sessions'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                last_update_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                data_list.append((last_update_dt, row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeMediaHistorySessions"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Media History - Sessions data available')
        db.close()

    return all_data_headers, all_data, report_file

@artifact_processor
def chromeMediaHistoryPlaybacks(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Updated', 'ID', 'Origin ID', 'URL', 'Watch Time', 'Has Audio', 'Has Video']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')
    lava_data_headers[4] = (lava_data_headers[4], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not file_found.endswith('Media History'):
            continue  # Skip all other files

        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        select
            datetime(last_updated_time_s-11644473600, 'unixepoch') as last_updated_time_s,
            id,
            origin_id,
            url,
            strftime('%H:%M:%S',watch_time_s, 'unixepoch') as watch_time_s,
            case has_audio
                when 0 then ''
                when 1 then 'Yes'
            end as has_audio,
            case has_video
                when 0 then ''
                when 1 then 'Yes'
            end as has_video  
        from playback
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Media History - Playbacks'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                last_update_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                watch_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[4]), timezone_offset)
                data_list.append((last_update_dt, row[1], row[2], row[3], watch_dt, row[5], row[6]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeMediaHistoryPlaybacks"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Media History - Playbacks data available')
        db.close()

    return all_data_headers, all_data, report_file

@artifact_processor
def chromeMediaHistoryOrigins(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['Last Updated', 'ID', 'Origin',  'Aggregate Watchtime']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not file_found.endswith('Media History'):
            continue  # Skip all other files

        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()

        cursor.execute('''
        select
            datetime(last_updated_time_s-11644473600, 'unixepoch') as last_updated_time_s,
            id,
            origin,
            cast(aggregate_watchtime_audio_video_s/86400 as integer) || ':' || strftime('%H:%M:%S', aggregate_watchtime_audio_video_s ,'unixepoch') as aggregate_watchtime_audio_video_s
        from origin
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Media History - Origins'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()
            data_list = []
            for row in all_rows:
                last_update_dt = convert_utc_human_to_timezone(convert_ts_human_to_utc(row[0]), timezone_offset)
                data_list.append((last_update_dt, row[1], row[2], row[3]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeMediaHistoryOrigins"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Media History - Origins data available')

        db.close()

    return all_data_headers, all_data, report_file


@artifact_processor
def chromeNetworkActionPredictor(files_found, report_folder, seeker, wrap_text, timezone_offset):
    # all_data will be a consolidated list of all browsers with an extra column to discriminate the browser
    all_data = []

    data_headers = ['User Text', 'URL', 'Number of Hits', 'Number of Misses']

    lava_data_headers = data_headers.copy()
    lava_data_headers[0] = (lava_data_headers[0], 'datetime')

    all_data_headers = lava_data_headers + ['Browser Name']

    report_file = 'Unknown'

    for file_found in files_found:
        file_found = str(file_found)
        if not file_found.endswith('Network Action Predictor'):
            continue  # Skip all other files

        browser_name = get_browser_name(file_found)
        if file_found.find('app_sbrowser') >= 0:
            browser_name = 'Browser'

        report_file = file_found if report_file == 'Unknown' else report_file + ', ' + file_found

        db = open_sqlite_db_readonly(file_found)
        cursor = db.cursor()
        cursor.execute('''
        select
        user_text,
        url,
        number_of_hits,
        number_of_misses
        from network_action_predictor
        ''')

        all_rows = cursor.fetchall()
        if len(all_rows) > 0:
            report_name = f'{browser_name} - Network Action Predictor'
            report = ArtifactHtmlReport(report_name)
            # check for existing and get next name for report file, so report from another file does not get overwritten
            report_path = os.path.join(report_folder, f'{report_name}.temphtml')
            report_path = get_next_unused_name(report_path)[:-9]  # remove .temphtml
            report.start_artifact_report(report_folder, os.path.basename(report_path))
            report.add_script()

            data_list = []
            for row in all_rows:
                data_list.append((row[0], row[1], row[2], row[3]))

            report.write_artifact_data_table(data_headers, data_list, file_found)
            report.end_artifact_report()

            # Generate LAVA output

            category = "Chromium"
            module_name = "chromeNetworkActionPredictor"

            table_name, object_columns, column_map = lava_process_artifact(category, module_name, report_name,
                                                                           lava_data_headers, len(data_list))

            lava_insert_sqlite_data(table_name, data_list, object_columns, lava_data_headers, column_map)

            # Add browser name column to the data
            data_list = [row + (browser_name,) for row in data_list]

            # Add current list to the combined list
            all_data.extend(data_list)

        else:
            logfunc(f'No {browser_name} - Network Action Predictor data available')

        db.close()

    return all_data_headers, all_data, report_file
